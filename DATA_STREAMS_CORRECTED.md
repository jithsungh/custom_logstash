# âœ… CORRECTED: Dynamic Data Streams - The RIGHT Way

## ğŸš¨ **I Was Wrong - Here's What Actually Works**

---

## âŒ **What I Incorrectly Recommended**

```ruby
output {
  elasticsearch {
    data_stream_dataset => "%{[container_name]}"  # âŒ DOESN'T WORK!
  }
}
```

**Why it's wrong:**

- `data_stream_dataset` config option does NOT support sprintf
- It's treated as a literal string
- Logstash will either fail validation or create a data stream literally named `%{[container_name]}`

**Source:** [Elastic Discussion](https://discuss.elastic.co/t/dynamic-naming-of-elasticsearch-data-streams/325278)

---

## âœ… **The CORRECT Approach**

### **Method: Set Event Fields in Filter + Auto-Routing**

```ruby
filter {
  # 1. Validate and normalize container_name
  if ![container_name] {
    mutate {
      add_field => { "container_name" => "default" }
    }
  }

  # 2. Normalize for ES data stream naming rules
  mutate {
    lowercase => ["container_name"]
    gsub => ["container_name", "[^a-z0-9_-]", "_"]
  }

  # 3. Set data_stream fields ON THE EVENT
  mutate {
    add_field => {
      "[data_stream][type]" => "logs"
      "[data_stream][dataset]" => "%{[container_name]}"  # âœ… sprintf WORKS here!
      "[data_stream][namespace]" => "default"
    }
  }
}

output {
  elasticsearch {
    data_stream => true
    data_stream_auto_routing => true  # âœ… Use event's data_stream fields
    data_stream_sync_fields => true
    # DON'T set data_stream_dataset/type/namespace in output!
  }
}
```

---

## ğŸ”‘ **Key Principles**

### **1. Sprintf Works in Filters, Not Output Config**

| Location          | Sprintf Support | Example                                            |
| ----------------- | --------------- | -------------------------------------------------- |
| **Filter stage**  | âœ… YES          | `"%{[container_name]}"` â†’ `"nginx"`                |
| **Output config** | âŒ NO           | `data_stream_dataset => "%{...}"` â†’ literal string |

### **2. Auto-Routing Uses Event Fields**

When `data_stream_auto_routing => true`:

- Logstash reads `event.get("[data_stream][type]")`
- Logstash reads `event.get("[data_stream][dataset]")`
- Logstash reads `event.get("[data_stream][namespace]")`
- Routes event to data stream: `{type}-{dataset}-{namespace}`

### **3. Output Config is Fallback Only**

```ruby
output {
  elasticsearch {
    data_stream => true
    data_stream_type => "logs"        # â† Fallback if event field missing
    data_stream_dataset => "default"  # â† Fallback if event field missing
    data_stream_auto_routing => true  # â† Prefers event fields over config
  }
}
```

---

## ğŸ“‹ **Complete Working Example**

See: `examples/data_stream_dynamic_correct.conf`

### **Test Events**

```json
{"message": "nginx started", "container_name": "nginx", "level": "info"}
{"message": "app error", "container_name": "my-app", "level": "error"}
{"message": "db query", "container_name": "postgres_db", "level": "debug"}
```

### **Result**

```
Event 1 (nginx):
  [data_stream][type]      = "logs"
  [data_stream][dataset]   = "nginx"
  [data_stream][namespace] = "default"
  â†’ Routed to: logs-nginx-default

Event 2 (my-app):
  [data_stream][type]      = "logs"
  [data_stream][dataset]   = "my_app"  â† normalized (hyphen â†’ underscore)
  [data_stream][namespace] = "default"
  â†’ Routed to: logs-my_app-default

Event 3 (postgres_db):
  [data_stream][type]      = "logs"
  [data_stream][dataset]   = "postgres_db"
  [data_stream][namespace] = "default"
  â†’ Routed to: logs-postgres_db-default
```

---

## âš ï¸ **Critical Requirements**

### **1. ES Data Stream Naming Rules**

**Valid characters:**

- Lowercase letters: `a-z`
- Numbers: `0-9`
- Underscore: `_`
- Hyphen: `-` (with restrictions)

**Restrictions:**

- Must be lowercase
- Cannot start with `_`, `-`, or `+`
- Cannot be `.` or `..`
- Cannot contain `\`, `/`, `*`, `?`, `"`, `<`, `>`, `|`, ` ` (space), `,`, `#`

**Our normalization handles this:**

```ruby
mutate {
  lowercase => ["container_name"]
  gsub => ["container_name", "[^a-z0-9_-]", "_"]
}

ruby {
  code => '
    container = event.get("container_name")
    container = container.sub(/^[-_]+/, "")  # Remove leading - or _
    container = "default" if container.empty?
    event.set("container_name", container)
  '
}
```

---

### **2. Index Template Required**

Data streams need a matching index template.

**Create a wildcard template:**

```bash
curl -X PUT "eck-es-http:9200/_index_template/logs-template?pretty" \
  -u elastic:password \
  -H 'Content-Type: application/json' \
  -d'{
  "index_patterns": ["logs-*-*"],
  "data_stream": {},
  "priority": 200,
  "template": {
    "settings": {
      "index.lifecycle.name": "logs"
    },
    "mappings": {
      "properties": {
        "@timestamp": {"type": "date"},
        "message": {"type": "text"},
        "container_name": {"type": "keyword"},
        "level": {"type": "keyword"}
      }
    }
  }
}'
```

**This template:**

- Matches all data streams: `logs-*-*`
- Enables data stream mode: `"data_stream": {}`
- Attaches ILM policy: `"index.lifecycle.name": "logs"`

---

### **3. ILM Policy for Rollover**

```bash
curl -X PUT "eck-es-http:9200/_ilm/policy/logs?pretty" \
  -u elastic:password \
  -H 'Content-Type: application/json' \
  -d'{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_primary_shard_size": "50gb",
            "max_age": "7d"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": {"number_of_shards": 1},
          "forcemerge": {"max_num_segments": 1}
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}'
```

---

### **4. ECS Compatibility**

Data streams expect ECS-compatible fields.

```ruby
output {
  elasticsearch {
    ecs_compatibility => "v8"  # â† Required!
    data_stream => true
  }
}
```

---

## ğŸ”¥ **Cardinality Warning**

### **The Problem**

Each unique `container_name` â†’ one data stream â†’ one set of backing indices.

**With 1000 unique containers:**

- 1000 data streams
- 1000+ backing indices (depending on rollover)
- 1000 ILM policy executions
- Heavy cluster metadata overhead

### **When It's Okay**

- **< 50 unique datasets:** Fine
- **50-200 unique datasets:** Manageable, but monitor cluster state
- **> 200 unique datasets:** Consider alternatives

### **Alternatives for High Cardinality**

#### **Option 1: Group Containers**

Instead of per-container data streams, group by category:

```ruby
filter {
  # Map container to category
  if [container_name] =~ /^nginx/ {
    mutate { add_field => { "service_type" => "web" } }
  } else if [container_name] =~ /^postgres|^mysql/ {
    mutate { add_field => { "service_type" => "database" } }
  } else {
    mutate { add_field => { "service_type" => "app" } }
  }

  # Use category for data stream
  mutate {
    add_field => {
      "[data_stream][dataset]" => "%{[service_type]}"
    }
  }

  # Keep container_name as a field for filtering
}
```

**Result:** 3 data streams instead of 1000.

---

#### **Option 2: Use Custom Index (Not Data Stream)**

Go back to the fixed custom ILM implementation:

```ruby
output {
  elasticsearch {
    ilm_enabled => true
    ilm_policy => "common-ilm-policy"
    ilm_pattern => "{now/d}-000001"
    ilm_rollover_alias => "%{[container_name]}"  # Your fixed code
  }
}
```

**Trade-offs:**

- âœ… Handles high cardinality
- âš ï¸ Custom code (may break on upgrades)
- âš ï¸ More complex to maintain

---

## ğŸ“Š **Comparison: Auto-Routing vs Custom ILM**

| Feature               | Auto-Routing Data Streams  | Custom ILM (Your Code)  |
| --------------------- | -------------------------- | ----------------------- |
| **Sprintf in output** | âŒ No (use filters)        | âœ… Yes (with my fixes)  |
| **Official support**  | âœ… Yes                     | âŒ No                   |
| **ILM attached**      | âœ… Yes (via template)      | âœ… Yes (via code)       |
| **Cardinality limit** | âš ï¸ ~200 datasets           | âœ… Higher (1000+)       |
| **Setup complexity**  | ğŸŸ¢ Low (filter + template) | ğŸŸ¡ Medium (custom code) |
| **Upgrade risk**      | ğŸŸ¢ None                    | ğŸ”´ High                 |
| **Maintenance**       | ğŸŸ¢ None                    | ğŸ”´ High                 |

---

## âœ… **Corrected Recommendations**

### **For Your Use Case:**

Based on your config:

```ruby
ilm_rollover_alias => "%{[container_name]}"
```

### **If < 100 Unique Containers:**

**Use Auto-Routing Data Streams** (see `examples/data_stream_dynamic_correct.conf`)

### **If 100-500 Unique Containers:**

**Option A:** Group containers into categories (10-20 data streams)

**Option B:** Use the fixed custom ILM implementation (but understand the risks)

### **If > 500 Unique Containers:**

**Use the fixed custom ILM implementation** - data streams aren't designed for this scale.

---

## ğŸ§ª **Testing the Correct Config**

### **1. Create Index Template**

```bash
curl -X PUT "eck-es-http:9200/_index_template/logs-template?pretty" \
  -u elastic:password \
  -H 'Content-Type: application/json' \
  -d'{
  "index_patterns": ["logs-*-*"],
  "data_stream": {},
  "priority": 200,
  "template": {
    "settings": {
      "index.lifecycle.name": "logs"
    }
  }
}'
```

### **2. Create ILM Policy**

```bash
curl -X PUT "eck-es-http:9200/_ilm/policy/logs?pretty" \
  -u elastic:password \
  -H 'Content-Type: application/json' \
  -d'{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {"max_age": "7d", "max_primary_shard_size": "50gb"}
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {"delete": {}}
      }
    }
  }
}'
```

### **3. Run Logstash**

```bash
bin/logstash -f examples/data_stream_dynamic_correct.conf
```

### **4. Send Test Events**

```bash
echo '{"message":"test nginx","container_name":"nginx"}' | bin/logstash -f examples/data_stream_dynamic_correct.conf
```

### **5. Verify Data Stream Created**

```bash
curl -u elastic:password "eck-es-http:9200/_data_stream/logs-nginx-default?pretty"

# Should show:
{
  "data_streams": [{
    "name": "logs-nginx-default",
    "backing_indices": [
      ".ds-logs-nginx-default-2025.11.14-000001"
    ],
    "generation": 1,
    "status": "green",
    "ilm_policy": "logs"
  }]
}
```

---

## ğŸ“ **Summary of Corrections**

### **What I Got Wrong:**

1. âŒ Said `data_stream_dataset => "%{...}"` works in output
2. âŒ Didn't mention auto-routing as the correct mechanism
3. âŒ Didn't warn about cardinality limits
4. âŒ Didn't explain naming rules properly

### **What's Actually Correct:**

1. âœ… Use filters with sprintf to set `[data_stream][dataset]`
2. âœ… Enable `data_stream_auto_routing => true`
3. âœ… Validate and normalize field values for ES naming rules
4. âœ… Understand cardinality limits (< 200 datasets recommended)
5. âœ… Create wildcard index templates for data streams
6. âœ… Use ECS compatibility

---

## ğŸ¯ **Final Recommendation**

**For your specific config:**

```ruby
ilm_rollover_alias => "%{[container_name]}"
```

### **Best Path Forward:**

1. **If < 100 containers:** Use auto-routing data streams (filter approach)
2. **If > 100 containers:** Stick with your fixed custom ILM implementation

**Both work. Choose based on scale and maintenance preferences.**

---

**Thank you for the correction! This is the right way to do it.**
